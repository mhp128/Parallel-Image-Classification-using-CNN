{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from numba import cuda\n",
        "import time"
      ],
      "metadata": {
        "id": "aTaJivV_W9Zz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils Function"
      ],
      "metadata": {
        "id": "Rq0QC2WeXnkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolution"
      ],
      "metadata": {
        "id": "yLy6jU_KWsBM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yS45eKWMfp1Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Layer():\n",
        "  def forward(self,inputs):\n",
        "    pass\n",
        "  def backward(self,output_gradient, learning_rate):\n",
        "    pass\n",
        "\n",
        "@cuda.jit\n",
        "def conv_forward_kernel(input,weights,stride, output):\n",
        "  n_chanels=input.shape[2]\n",
        "  n_filters,filter_size,_,__=weights.shape\n",
        "  output_height,output_width,_ = output.shape\n",
        "  row,col,fillterIdx = cuda.grid(3)\n",
        "  if(row>=output_height or col>=output_width or fillterIdx>=n_filters):\n",
        "    return\n",
        "  sum=0\n",
        "  for chanel_idx in range(n_chanels):\n",
        "      for fillterRow in range(filter_size):\n",
        "        for fillterCol in range(filter_size):\n",
        "            iR = row*stride + fillterRow\n",
        "            iC = col*stride + fillterCol\n",
        "            sum+=input[iR,iC,chanel_idx]*weights[fillterIdx,fillterRow,fillterCol,chanel_idx]\n",
        "  output[ row, col,fillterIdx] = sum\n",
        "  \n",
        "\n",
        "@cuda.jit\n",
        "def conv_backward_kernel(input,weights,stride, output):\n",
        "  n_chanels=input.shape[-1]\n",
        "  n_filters,filter_size,_,__=weights.shape\n",
        "  output_height,output_width,_ = output.shape\n",
        "  row,col,fillterIdx = cuda.grid(3)\n",
        "  if(row>=output_height or col>=output_width or fillterIdx>=n_filters):\n",
        "    return\n",
        "  sum=0\n",
        "  for chanel_idx in range(n_chanels):\n",
        "      for fillterRow in range(filter_size):\n",
        "        for fillterCol in range(filter_size):\n",
        "            iR = row*stride + fillterRow\n",
        "            iC = col*stride + fillterCol\n",
        "            input_gradient[ iR, iC,i_chanel] += (\n",
        "                  self.weights[fillterIdx,iR,iC,i_chanel] * output_gradient[ row, col,filter_index])\n",
        "            filter_gradient[fillterIdx,iR,iC,i_chanel] += (\n",
        "                  self.inputs[ iR, iC,i_chanel] * output_gradient[row, col,fillterIdx])\n",
        "  output[ row, col,fillterIdx] = sum\n",
        "  pass\n",
        "\n",
        "class Convolution(Layer):\n",
        "  def __init__(self,n_filters=32,filter_size=3 ,stride=1,activation=None,input_shape=(28,28,1)):\n",
        "      self.n_filters = n_filters\n",
        "      self.filter_size = filter_size\n",
        "      self.stride = stride\n",
        "      self.activation = activation\n",
        "      np.random.seed(10)\n",
        "      # self.filters = np.random.randn(n_filters, filter_size, filter_size) / (filter_size * filter_size)\n",
        "      self.weights = np.random.randn(n_filters,filter_size, filter_size,input_shape[2])/  np.sqrt(2 / (input_shape[2] * filter_size * filter_size))\n",
        "      self.bias = np.zeros((n_filters, 1))\n",
        "      \n",
        "      self.inputs=None\n",
        "      self.use_device=False\n",
        "  def forward(self,inputs):\n",
        "      self.inputs =inputs\n",
        "      n_batchs, in_height,in_width,n_chanels=inputs.shape\n",
        "      output_width = int((in_width - self.filter_size ) / self.stride) + 1\n",
        "      output_height = int((in_height - self.filter_size ) / self.stride) + 1\n",
        "      outputs = np.zeros( (n_batchs,output_height,output_width,  self.n_filters))\n",
        "      start= time.time()\n",
        "      ## ===========USING CPU===========##\n",
        "      if(self.use_device==False):\n",
        "        for i_idx in range(n_batchs):\n",
        "          for row in range(output_height):\n",
        "            for col in range(output_width):\n",
        "              for f_idx in range(self.n_filters):\n",
        "                        row_start = row * self.stride\n",
        "                        row_end = row_start + self.filter_size\n",
        "                        col_start = col * self.stride\n",
        "                        col_end = col_start + self.filter_size\n",
        "                        sum=0\n",
        "                        for in_chanel in range(self.weights.shape[-1]):\n",
        "                          sum+= np.sum( self.weights[f_idx,:,:,in_chanel]* inputs[i_idx,  row_start:row_end, col_start:col_end,in_chanel])\n",
        "                        outputs[i_idx, row, col,f_idx] =sum\n",
        "      ## ===========USING DEVICE===========##\n",
        "      else:\n",
        "        block_size = (4,4,4)\n",
        "        grid_size = ((output_height-1)//block_size[0]+1,(output_width-1)//block_size[1]+1,(self.n_filters-1)//block_size[2]+1)\n",
        "        out_device= cuda.to_device(outputs[0])\n",
        "\n",
        "        weights_device = cuda.to_device(self.weights)\n",
        "        for i in range(n_batchs):\n",
        "          inputs_device = cuda.to_device(inputs[i])\n",
        "          conv_forward_kernel[grid_size,block_size](inputs_device,weights_device,1,out_device)\n",
        "          outputs[i]=out_device.copy_to_host()\n",
        "      ## ===========END USING DEVICE===========##\n",
        "      if(self.activation==\"relu\"):\n",
        "        outputs = np.maximum(0,outputs)\n",
        "      end= time.time()\n",
        "      print(\"time:\",end-start )\n",
        "      return outputs\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "      n_batchs, input_height, input_width,input_channels = self.inputs.shape\n",
        "      _,  output_height, output_width,num_filters = output_gradient.shape\n",
        "      filter_gradient = np.zeros(self.weights.shape)\n",
        "      input_gradient =  np.zeros(self.inputs.shape)\n",
        "\n",
        "      for image_index in range(n_batchs):\n",
        "          for filter_index in range(num_filters):\n",
        "              for row in range(output_height):\n",
        "                  for col in range(output_width):\n",
        "                      row_start = row * self.stride\n",
        "                      row_end = row_start + self.filter_size\n",
        "                      col_start = col * self.stride\n",
        "                      col_end = col_start + self.filter_size\n",
        "                      for i_chanel in range(inputs.shape[-1]):\n",
        "                        input_gradient[image_index,  row_start:row_end, col_start:col_end,i_chanel] += (\n",
        "                              self.weights[filter_index,:,:,i_chanel] * output_gradient[image_index, row, col,filter_index])\n",
        "                        filter_gradient[filter_index,:,:,i_chanel] += (\n",
        "                              self.inputs[image_index, row_start:row_end, col_start:col_end,i_chanel] * output_gradient[\n",
        "                          image_index, row, col,filter_index])\n",
        "\n",
        "      self.weights -= learning_rate * filter_gradient/n_batchs\n",
        "      if(self.activation==\"relu\"):\n",
        "        input_gradient[self.inputs <= 0] = 0\n",
        "      return input_gradient\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=np.random.randint(1,100, (10,28,28,1))\n",
        "conv =Convolution(n_filters=32, filter_size=3, stride=1,activation='relu',input_shape=(28,28,1))\n",
        "outputs_host=conv.forward(inputs)\n",
        "conv.use_device=True\n",
        "outputs_device=conv.forward(inputs)\n",
        "# np.sum(np.abs(outputs_device-outputs_host))\n",
        "\n",
        "input_grad=conv.backward(outputs_device,0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZvuSG7dnDirT",
        "outputId": "d66cc6d1-2e98-4efd-b81f-4bebf1dcd5fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ed142a7fbc83>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mConvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutputs_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutputs_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0dc451d0706a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;31m## ===========END USING DEVICE===========##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreLu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reLu' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "BMDNwamv7fwR",
        "outputId": "5f88cd3f-3e85-49cb-cfd2-55fef37a3144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-211-7d1ad2993579>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-209-cf86a2b2bb95>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, output_gradient, learning_rate)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           image_index, row, col,filter_index])\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfilter_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0minput_gradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Convolution' object has no attribute 'filters'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conv.use_device=True\n",
        "outputs_device=conv.forward(inputs)\n",
        "conv.use_device=False\n",
        "outputs_host=conv.forward(inputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0pyRFDYp44Q",
        "outputId": "9109ef3c-42e2-4920-c2e1-cb98fe075421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time excute: 1.2260425090789795\n",
            "time excute: 15.554473161697388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"loss=\",np.sum(np.abs(outputs_host-outputs_device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKv0VJRKBqNm",
        "outputId": "9362deba-89f9-4888-c11d-369f415d9f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss= 1.863692968072199e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU"
      ],
      "metadata": {
        "id": "4jLGW5tg5OIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max Pooling\n"
      ],
      "metadata": {
        "id": "rN006s-b19IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def maxPool2D_forward_kernel(inputs, outputs,stride,pool_size):\n",
        "  n_batchs,in_height,in_width,n_chanels=inputs.shape\n",
        "  n_batchs,output_height,output_width,_ = outputs.shape\n",
        "  ibatch,h,w = cuda.grid(3)\n",
        "          # Max pool over input\n",
        "  if(ibatch>=n_batchs or h>=output_height or w>=output_width): return\n",
        "  h_start = h * stride\n",
        "  h_end = h_start + pool_size\n",
        "  w_start = w * stride\n",
        "  w_end = w_start + pool_size\n",
        "  for i_chanel in range(n_chanels):\n",
        "    # findmax\n",
        "    slice_input=inputs[ibatch,h_start:h_end, w_start:w_end,i_chanel]\n",
        "    max_value=slice_input[0,0]\n",
        "    for h_pool in range(pool_size):\n",
        "      for w_pool in range(pool_size):\n",
        "        max_value=max(max_value,slice_input[h_pool,w_pool])\n",
        "\n",
        "    outputs[ibatch,h, w,i_chanel] =max_value\n",
        "\n",
        "\n",
        "class MaxPool2D(Layer):\n",
        "    def __init__(self, pool_size=2, stride=2):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "        self.use_device=False\n",
        "        self.inputs=None\n",
        "    def forward(self, inputs):\n",
        "        # Save input\n",
        "        self.inputs = inputs\n",
        "        batch_size, input_height, input_width,num_channels = inputs.shape\n",
        "        output_height = int((input_height - self.pool_size) / self.stride) + 1\n",
        "        output_width = int((input_width - self.pool_size) / self.stride) + 1\n",
        "        \n",
        "        # Initialize output\n",
        "        start= time.time()\n",
        "        ## ===========USING CPU===========##\n",
        "\n",
        "        outputs =np.zeros((batch_size, output_height, output_width,num_channels))\n",
        "        if(self.use_device==False):\n",
        "          # Max pool over input\n",
        "          for h in range(output_height):\n",
        "              for w in range(output_width):\n",
        "                  h_start = h * self.stride\n",
        "                  h_end = h_start + self.pool_size\n",
        "                  w_start = w * self.stride\n",
        "                  w_end = w_start + self.pool_size\n",
        "                  outputs[:,h, w,:] = np.max(inputs[:, h_start:h_end, w_start:w_end,:], axis=(1, 2))\n",
        "        ## ===========USING DEVICE===========##\n",
        "        else:\n",
        "          out_device= cuda.device_array_like(outputs)\n",
        "          block_size = (8,4,4)\n",
        "          grid_size = ((batch_size-1)//block_size[0]+1,(output_height-1)//block_size[1]+1,(output_width-1)//block_size[2]+1)\n",
        "          inputs_device = cuda.to_device(inputs)\n",
        "          maxPool2D_forward_kernel[grid_size,block_size](inputs_device,out_device,self.stride,self.pool_size)\n",
        "          outputs=out_device.copy_to_host()\n",
        "      ## ===========END USING DEVICE===========##\n",
        "        end = time.time()\n",
        "        print('time=',end-start)\n",
        "        return outputs\n",
        "    \n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        # Initialize input error\n",
        "        input_gradient = np.zeros(self.inputs.shape)\n",
        "        batch_size, output_height, output_width,num_channels = output_gradient.shape\n",
        "        # Max pool over input gradients\n",
        "        for h in range(output_height):\n",
        "            for w in range(output_width):\n",
        "                h_start = h * self.stride\n",
        "                h_end = h_start + self.pool_size\n",
        "                w_start = w * self.stride\n",
        "                w_end = w_start + self.pool_size\n",
        "                input_slice = self.inputs[:, h_start:h_end, w_start:w_end,:]\n",
        "                max_vals = np.max(input_slice, axis=(1, 2), keepdims=True)\n",
        "                max_mask = (input_slice == max_vals)\n",
        "                input_gradient[:,h_start:h_end, w_start:w_end,:] += max_mask * output_gradient[:,  h, w,:, np.newaxis, np.newaxis]\n",
        "        \n",
        "        return input_gradient\n"
      ],
      "metadata": {
        "id": "aD299xBoW3Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=np.random.randint(1,10, (100,200, 200,32))"
      ],
      "metadata": {
        "id": "ozpzGIInM6No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pool = MaxPool2D()\n",
        "out=pool.forward(inputs)\n",
        "pool.use_device=True\n",
        "out1= pool.forward(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj3t1QM6MtQK",
        "outputId": "60535245-984c-4a6a-9014-f0b7fe497bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time= 0.989051342010498\n",
            "time= 0.6579642295837402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum( np.abs(out - out1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KArDGa3Ddlj9",
        "outputId": "7880d20a-ccc4-4203-be69-f376b2353c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense"
      ],
      "metadata": {
        "id": "jnjT5Q5aWzzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A=np.random.randint(1,2, (3,3,2))\n",
        "B=np.random.randint(1,2, (3,3,2))\n",
        "\n",
        "Expect:  A*B <=> A[...,0]+B[...,B]"
      ],
      "metadata": {
        "id": "xEhmWs5LJHLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CPU"
      ],
      "metadata": {
        "id": "Wl0nHomHA1Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda.jit\n",
        "def dense_forward_device(inputs,weights,bias,outputs):\n",
        "  row,col= cuda.grid(2)\n",
        "  height = weights.shape[0] \n",
        "  if(row>=inputs.shape[0] or col >= outputs.shape[1] ): return \n",
        "  sum=0\n",
        "  for i in range(inputs.shape[1]):\n",
        "    sum+= inputs[row,i] *  weights[i,col]\n",
        "  outputs[row,col] = sum +bias[0,col]\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, num_inputs, num_outputs, activation=\"relu\"):\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        np.random.seed(10)\n",
        "        self.weights = np.random.randn(num_inputs, num_outputs) * np.sqrt(1. / num_inputs)\n",
        "        np.random.seed(10)\n",
        "        self.biases = np.zeros((1, num_outputs))\n",
        "        self.activation= activation\n",
        "        self.use_device=False \n",
        "        self.inputs  =None\n",
        "    def forward(self, inputs):\n",
        "        # Save input\n",
        "        self.inputs = inputs\n",
        "        \n",
        "        # Compute output\n",
        "        start =time.time()\n",
        "        outputs=np.zeros((inputs.shape[0],self.weights.shape[1]))\n",
        "        if(self.use_device==False):\n",
        "          outputs = np.dot(inputs, self.weights)+ self.biases\n",
        "        else:\n",
        "          weights_device = cuda.to_device(self.weights)\n",
        "          biases_device = cuda.to_device(self.biases)\n",
        "          out_device= cuda.device_array_like(outputs)\n",
        "          block_size = (32,16)\n",
        "          grid_size = ((inputs.shape[0]-1)//block_size[0]+1,(inputs.shape[0]-1)//block_size[1]+1)\n",
        "          inputs_device = cuda.to_device(inputs)\n",
        "          dense_forward_device[grid_size,block_size](inputs_device,weights_device,biases_device,out_device)\n",
        "          outputs = out_device.copy_to_host()\n",
        "        end = time.time()\n",
        "        print('time=',end-start)\n",
        "\n",
        "        if(self.activation==\"relu\"):\n",
        "          outputs = np.maximum(0,outputs)\n",
        "        elif self.activation == \"softmax\":\n",
        "          outputs = self.softmax( outputs )\n",
        "        return outputs\n",
        "    def softmax(self,x):\n",
        "      e_x = np.exp(x - np.max(x))\n",
        "      return e_x / e_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        # Compute input error\n",
        "        input_error = np.dot(output_gradient, self.weights.T)\n",
        "        \n",
        "        # Compute gradients\n",
        "        weights_gradient = np.dot(self.inputs.T, output_gradient)\n",
        "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
        "        \n",
        "        # Update weights and biases\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.biases -= learning_rate * biases_gradient\n",
        "        \n",
        "        return input_error"
      ],
      "metadata": {
        "id": "12OrnbobW36y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeGWUl5osX_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=np.random.randint(1,100, (10000,10000))"
      ],
      "metadata": {
        "id": "CN3ETrzukBf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense=Dense(10000,100)\n",
        "out_host=dense.forward(inputs)\n",
        "dense.use_device=True\n",
        "out_device=dense.forward(inputs)\n",
        "out_host.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eRGhasXkNuD",
        "outputId": "183d344f-1764-4a1b-dee5-c404e1cf134d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time= 0.9002511501312256\n",
            "time= 0.6991419792175293\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.abs(out_host-out_device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjyoO9hasELo",
        "outputId": "f1bbe60b-700c-43dc-ace0-93113f392f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.12350361108875e-08"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU"
      ],
      "metadata": {
        "id": "oLEjPQKaA4pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(Layer):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def forward(self,inputs):\n",
        "    self.inputs=inputs\n",
        "    return inputs.reshape(inputs.shape[0], -1)\n",
        "  def backward(self,output_gradient,learning_rate):\n",
        "    shape= self.inputs.shape\n",
        "    return output_gradient.reshape(shape)"
      ],
      "metadata": {
        "id": "NtjkEFhQwcmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "jT0Cf2l24TQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel:\n",
        "  def __init__(self, layers:list[Layer]=[]):\n",
        "    self.layers:list[Layer]=layers\n",
        "    pass\n",
        "  def add(self,layer:Layer):\n",
        "    self.layers.append(layer)\n",
        "  def forward(self,X):\n",
        "    output= X\n",
        "    for layer in self.layers:\n",
        "            output=layer.forward(output)\n",
        "    return output\n",
        "\n",
        "  def fit(self,X_train,Y_train,epochs=1, batch_size=32, learning_rate=0.001, loss=\"CrossEntropy\"):\n",
        "    num_batch=(len(X_train)-1)/batch_size+1\n",
        "    for i_epoch in range(epochs):\n",
        "      train_loss =0\n",
        "      for i in range(num_batch-1):\n",
        "        batch_start = i * batch_size\n",
        "        batch_end = (i + 1) * batch_size\n",
        "        batch_X= X_train[batch_start: batch_end]\n",
        "        batch_Y= Y_train[batch_start: batch_end]\n",
        "        predictions=self.forward(batch_X)\n",
        "        train_loss += np.sum((predictions - batch_Y) ** 2)\n",
        "\n",
        "        d_L_d_predictions = 2.0 * (predictions - batch_Y)\n",
        "      # loss= y_train-y_predict\n",
        "      print(loss)\n",
        "      # for layer in self.layers:\n",
        "      #   Xinputs=layer.back(Xinputs)\n",
        "  def predict(self,X):\n",
        "    return self.forward(X)\n",
        "\n",
        "  def use_device(self, value):\n",
        "    for layer in self.layers:\n",
        "      output=layer.use_device=value\n",
        "  @staticmethod\n",
        "  def cross_entropy_loss(predictions, targets):\n",
        "      num_samples = predictions.shape[0]\n",
        "      softmax = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n",
        "      softmax /= np.sum(softmax, axis=1, keepdims=True)\n",
        "      log_likelihood = -np.log(softmax[range(num_samples), targets])\n",
        "      loss = np.sum(log_likelihood) / num_samples\n",
        "      return loss\n",
        "\n",
        "  @staticmethod\n",
        "  def cross_entropy_loss_derivative(predictions, targets):\n",
        "      num_samples = predictions.shape[0]\n",
        "      softmax = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n",
        "      softmax /= np.sum(softmax, axis=1, keepdims=True)\n",
        "      softmax[range(num_samples), targets] -= 1\n",
        "      softmax /= num_samples\n",
        "      return softmax"
      ],
      "metadata": {
        "id": "qxGdhWFdvDzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "assert x_train.shape == (60000, 28, 28)\n",
        "assert x_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)"
      ],
      "metadata": {
        "id": "0XTzbBrm1Ta2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a181b4d8-c1cb-466a-8fa8-2f685e281d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=CNNModel([ \n",
        "    Convolution(n_filters=32, filter_size=3, stride=1,activation='relu'),\n",
        "    Convolution(n_filters=64, filter_size=3, stride=1,activation='relu'),\n",
        "    MaxPool2D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(9216,128, activation='relu'),\n",
        "    Dense(128,10, activation='softmax')\n",
        "])\n",
        "model.use_device(True)\n"
      ],
      "metadata": {
        "id": "SRJ2zExSWS7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=np.random.randint(1,100, (100,28,28,1))"
      ],
      "metadata": {
        "id": "qmUDgfF2zjHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y=model.predict(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di6QhfFcz0v6",
        "outputId": "4ec2f5fc-2581-4c4e-b633-004fe67b748a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 0.0701451301574707\n",
            "time: 0.1884913444519043\n",
            "time= 0.014975786209106445\n",
            "time= 0.02480483055114746\n",
            "time= 0.001131296157836914\n",
            "CPU times: user 285 ms, sys: 22.8 ms, total: 308 ms\n",
            "Wall time: 307 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6J1uWba0pVU",
        "outputId": "986b4cf3-6375-4548-a884-ec494ee17b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    }
  ]
}